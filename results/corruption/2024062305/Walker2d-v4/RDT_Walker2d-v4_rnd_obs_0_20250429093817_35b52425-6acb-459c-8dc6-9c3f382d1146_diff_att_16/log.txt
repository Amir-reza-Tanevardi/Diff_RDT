Logging to results/corruption/2024062305/Walker2d-v4/RDT_Walker2d-v4_rnd_obs_0_20250429093817_35b52425-6acb-459c-8dc6-9c3f382d1146_diff_att_16
eval_every: 1
n_episodes: 10
device: cuda
num_epochs: 100
num_updates_on_epoch: 1000
use_diff_att: True
embedding_dim: 128
num_layers: 3
num_heads: 16
seq_len: 20
episode_len: 1000
attention_dropout: 0.0
residual_dropout: 0.1
embedding_dropout: None
mlp_embedding: False
mlp_head: False
mlp_reward: True
embed_order: rsa
learning_rate: 0.0001
betas: (0.9, 0.999)
weight_decay: 0.0001
clip_grad: 0.25
batch_size: 64
update_steps: 100000
warmup_steps: 10000
reward_scale: 0.001
normalize: True
normalize_reward: False
loss_fn: wmse
wmse_coef: (0.0, 0.0)
reward_coef: 1.0
recalculate_return: False
correct_freq: 1
correct_start: 50
correct_thershold: None
target_returns: (12000.0, 6000.0)
eval_id: 00
eval_only: False
eval_attack: True
eval_attack_eps: 0.01
eval_attack_mode: random
checkpoint_dir: None
use_wandb: 0
group: 2024062305
env: Walker2d-v4
minari_dataset_id: minari/walker2d-medium-v2
seed: 0
down_sample: True
sample_ratio: 0.1
debug: False
alg_type: RDT
logdir: results/corruption
dataset_path: your_dataset_path
save_model: True
corruption_agent: IQL
corruption_seed: 0
corruption_mode: random
corruption_obs: 1.0
corruption_act: 0.0
corruption_rew: 0.0
corruption_rate: 0.3
use_original: 0
same_index: 0
froce_attack: 0
logfile: RDT_Walker2d-v4_rnd_obs_0_20250429093817_35b52425-6acb-459c-8dc6-9c3f382d1146_diff_att_16
Load new dataset from your_dataset_path/log_attack_data/Walker2d-v4/random_0_ratio_0.1_obs_1.0_0.3.pth
random observations
Attack name: _ratio_0.1_obs_1.0_0.3
Dataset: 104 trajectories
State mean: [[ 1.22334051e+00 -1.67085797e-02 -4.14643609e-01 -6.83707640e-03
   2.56521932e-01 -9.76107965e-02 -5.49006676e-01  1.15862676e-01
   4.96850683e+00 -2.91792073e-03  1.98722032e-03 -4.33489993e-02
  -2.10986769e-02  6.35574327e-01 -1.25708428e-02  7.40375992e-02
   5.35483126e-01]], std: [[0.06329407 0.23992098 0.41358941 0.09799204 0.64112285 0.21625538
  0.4059761  0.67789316 1.40967205 0.67806036 2.35349475 3.93122688
  1.96666773 6.00912943 3.11459959 5.43827092 6.79833387]]
Network: 
DecisionTransformer(
  (emb_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (timestep_emb): Embedding(1020, 128)
  (state_emb): Linear(in_features=17, out_features=128, bias=True)
  (action_emb): Linear(in_features=6, out_features=128, bias=True)
  (return_emb): Linear(in_features=1, out_features=128, bias=True)
  (blocks): ModuleList(
    (0-2): 3 x TransformerBlock(
      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (drop): Dropout(p=0.1, inplace=False)
      (attention): MultiheadDiffAttn(
        (q_proj): Linear(in_features=128, out_features=128, bias=False)
        (k_proj): Linear(in_features=128, out_features=128, bias=False)
        (v_proj): Linear(in_features=128, out_features=128, bias=False)
        (out_proj): Linear(in_features=128, out_features=128, bias=False)
        (subln): RMSNorm(dim=16, eps=1e-05, elementwise_affine=True)
      )
      (mlp): Sequential(
        (0): Linear(in_features=128, out_features=512, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=512, out_features=128, bias=True)
        (3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (predict_dropout): Dropout(p=0.1, inplace=False)
  (action_head): MLPBlock(
    (model): Sequential(
      (0): Linear(in_features=128, out_features=6, bias=True)
      (1): Tanh()
    )
  )
  (reward_head): MLPBlock(
    (model): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=128, out_features=1, bias=True)
    )
  )
)
Total parameters: 745367

Traceback (most recent call last):
  File "/content/Diff_RDT/RDT.py", line 530, in main
    train(config, logger)
  File "/content/Diff_RDT/RDT.py", line 391, in train
    eval_log = dt_func.eval_fn(config, env, model)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/dt_functions.py", line 258, in eval_fn
    eval_return, eval_len = eval_rollout(
                            ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/dt_functions.py", line 228, in eval_rollout
    predicted = model(  # fix this noqa!!!
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/dt_functions.py", line 401, in forward
    out = block(out, padding_mask=padding_mask)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/networks.py", line 204, in forward
    attention_out = self.attention(
                    ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/multihead_diffattn.py", line 112, in forward
    q = apply_rotary_emb(q, *rel_pos, interleaved=True)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/rotary.py", line 360, in apply_rotary_emb
    return ApplyRotaryEmb.apply(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/rotary.py", line 285, in forward
    out = apply_rotary(
          ^^^^^^^^^^^^^
  File "/content/Diff_RDT/utils/rotary.py", line 179, in apply_rotary
    assert rotary_dim <= headdim, "rotary_dim must be <= headdim"
           ^^^^^^^^^^^^^^^^^^^^^
AssertionError: rotary_dim must be <= headdim

